<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Notes on Diffusion Models | Kun</title>
<meta name="keywords" content="">
<meta name="description" content="Notations $p(z|x)$: true posterior.
$q_\phi(z|x)$: approximate posterior with parameters $\phi$.
$x_0$: true data.
$x_t, t \in [1,T]$: latent variables.
$q(x_{t}|x_{t-1})$: forward process.
$p(x_t|x_{t&#43;1})$: reverse process.
Understanding Diffusion Models from the Perspective of VAE This section briefly summarizes @Luo2022 with my understanding, particularly focusing on the math logic of behind the diffusion models. The original article by Calvin Luo is highly recommended! It helps a lot!
Evidence Lower Bound What are likelihood-based generative models?">
<meta name="author" content="Xikun Huang">
<link rel="canonical" href="https://xikunhuang.github.io/posts/notes-on-diffusion-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d918aa2b3672274a508449848c9e6fcc030030e995cea61623654add81dfe9a7.css" integrity="sha256-2RiqKzZyJ0pQhEmEjJ5vzAMAMOmVzqYWI2VK3YHf6ac=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://xikunhuang.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://xikunhuang.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://xikunhuang.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://xikunhuang.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://xikunhuang.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

<meta property="og:title" content="Notes on Diffusion Models" />
<meta property="og:description" content="Notations $p(z|x)$: true posterior.
$q_\phi(z|x)$: approximate posterior with parameters $\phi$.
$x_0$: true data.
$x_t, t \in [1,T]$: latent variables.
$q(x_{t}|x_{t-1})$: forward process.
$p(x_t|x_{t&#43;1})$: reverse process.
Understanding Diffusion Models from the Perspective of VAE This section briefly summarizes @Luo2022 with my understanding, particularly focusing on the math logic of behind the diffusion models. The original article by Calvin Luo is highly recommended! It helps a lot!
Evidence Lower Bound What are likelihood-based generative models?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://xikunhuang.github.io/posts/notes-on-diffusion-models/" /><meta property="og:image" content="https://xikunhuang.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-06-01T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://xikunhuang.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Notes on Diffusion Models"/>
<meta name="twitter:description" content="Notations $p(z|x)$: true posterior.
$q_\phi(z|x)$: approximate posterior with parameters $\phi$.
$x_0$: true data.
$x_t, t \in [1,T]$: latent variables.
$q(x_{t}|x_{t-1})$: forward process.
$p(x_t|x_{t&#43;1})$: reverse process.
Understanding Diffusion Models from the Perspective of VAE This section briefly summarizes @Luo2022 with my understanding, particularly focusing on the math logic of behind the diffusion models. The original article by Calvin Luo is highly recommended! It helps a lot!
Evidence Lower Bound What are likelihood-based generative models?"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://xikunhuang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Notes on Diffusion Models",
      "item": "https://xikunhuang.github.io/posts/notes-on-diffusion-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Notes on Diffusion Models",
  "name": "Notes on Diffusion Models",
  "description": "Notations $p(z|x)$: true posterior.\n$q_\\phi(z|x)$: approximate posterior with parameters $\\phi$.\n$x_0$: true data.\n$x_t, t \\in [1,T]$: latent variables.\n$q(x_{t}|x_{t-1})$: forward process.\n$p(x_t|x_{t+1})$: reverse process.\nUnderstanding Diffusion Models from the Perspective of VAE This section briefly summarizes @Luo2022 with my understanding, particularly focusing on the math logic of behind the diffusion models. The original article by Calvin Luo is highly recommended! It helps a lot!\nEvidence Lower Bound What are likelihood-based generative models?",
  "keywords": [
    
  ],
  "articleBody": "Notations $p(z|x)$: true posterior.\n$q_\\phi(z|x)$: approximate posterior with parameters $\\phi$.\n$x_0$: true data.\n$x_t, t \\in [1,T]$: latent variables.\n$q(x_{t}|x_{t-1})$: forward process.\n$p(x_t|x_{t+1})$: reverse process.\nUnderstanding Diffusion Models from the Perspective of VAE This section briefly summarizes @Luo2022 with my understanding, particularly focusing on the math logic of behind the diffusion models. The original article by Calvin Luo is highly recommended! It helps a lot!\nEvidence Lower Bound What are likelihood-based generative models? What are autoregressive generative models? The transformer family belongs to autoregressive models.\nFor likelihood-based generative models, we want to model the true data distribution by maximizing the likelihood $p(x)$ of all observed data $x$. We can think of the data we observed as represented or generated by an associated unseen latent variable, which can be denoted by random variable $z$. There are two direct but difficult ways to compute the likelihood $p(x)$:\n$p(x) = \\int p(x,z)dz$: intractable for complex models since it involves integrating all latent variables.\n$p(x) = \\frac{p(x,z)}{p(z|x)}$: difficult since it involves having access to a ground truth $p(z|x)$.\nHere I have a question: How do you know the true joint distribution $p(x,z)$?\nHowever, we can derive a term called the Evidence Lower Bound (ELBO) using the above two equations. So what is the ELBO? As its name suggests, it is a lower bound of the evidence. But what is the evidence? The evidence is the log likelihood of the observed data. The relationship between the evidence and the ELBO can be mathematically written as:\n$$ \\begin{align} \\log p(x) \u0026= \\mathbb{E}{q\\phi(z|x)}[\\log \\frac{p(x,z)}{q_\\phi(z|x)}] + D_{KL}(q_\\phi(z|x)||p(z|x)) \\ \u0026 \\geq \\underbrace{\\mathbb{E}{q\\phi(z|x)}[\\log \\frac{p(x,z)}{q_\\phi(z|x)}]}_\\text{ELBO} \\end{align} $$\nWhy the ELBO is an objective we would like to maximize?\nThe ELBO is indeed a lower bound of the evidence.\nAny maximization of the ELBO term with respect $\\phi$ necessarily invokes an equal minimization of the $D_{KL}(q_\\phi(z|x)||p(z|x))$. Because the sum of the ELBO and the DL term is a constant with respect to $\\phi$ (the evidence $\\log p(x)$ does not depend on $\\phi$).\nAs we will see below, the ELBO can be further dissect into several interpretable components which can be computed analytically or approximated using existing estimate methods.\nVariational Autoencoders In the vanilla VAE, we directly maximize the ELBO. The ELBO can be further divided into two parts:\n$$ \\begin{equation} \\mathbb{E}{q\\phi(z|x)}[\\log \\frac{p(x,z)}{q_\\phi(z|x)}] = \\mathbb{E}{q\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z)) \\end{equation} $$\nHere $q_\\phi(z|x)$ can be regarded as the encoder with parameters $\\phi$ and $p_\\theta(x|z)$ can be regarded as the decoder with parameters $\\theta$. So there are two kinds of parameters to be optimized:\nEncoder parameters: $\\phi$ Decoder parameters: $\\theta$ The meaning of the name of VAE is:\nVariational: optimize for the best $q_\\phi(z|x)$ among a family of potential posterior distributions parameterized by $\\phi$. AutoEncoder: reminiscent of a traditional autoencoder model. How dose VAE optimize the ELBO jointly with parameters $\\phi$ and $\\theta$? To my understanding, VAE uses three techniques to do this:\nGaussian assumption.\nMonte Carlo estimate.\nReparameterization trick.\nThe encoder $q_\\phi(z|x)$ and the prior $p(z)$ are commonly chosen to be Gaussian, which makes the KL divergence term of the ELBO having closed-form expression, i.e.\n$$ \\begin{align} q_\\phi(z|x) \u0026= \\mathcal{N}(z; \\mu_\\phi(x), \\sigma_\\phi^2(x)I) \\ p(z) \u0026= \\mathcal{N}(z; 0, I) \\ D_{KL}(q_\\phi(z|x) || p(z)) \u0026 \\text{ can be computed analytically}. \\end{align} $$\nThe first term of the ELBO can be approximated using a Monte Carlo estimate, i.e.\n$$ \\mathbb{E}{q\\phi(z|x)}[\\log p_\\theta(x|z)] \\approx \\sum_{l=1}^{L}\\log p_{\\theta}(x|z^{(l)}) $$\nwhere ${z^{(l)}}{l=1}^{L}$ are sampled from $q\\phi(z|x)$ for every observation $x$ in the dataset, and $L$ is often set to be $1$ in practice.\nThe reparameterization trick writers a random variable as a deterministic function of a noise variable, and this allows for the optimization of the non-stochatic terms through gradient descent, i.e.\n$$ z = \\bm{\\mu}{\\bm{\\phi}}(x) + \\bm{\\sigma}{\\bm{\\phi}}(x)\\odot\\bm{\\epsilon} \\quad \\text{with } \\bm{\\epsilon} \\sim \\mathcal{N}(\\bm{\\epsilon};\\bm{0}, \\textbf{I}) $$\nThe objective function of VAE is ELBO which contains the reconstruction term $\\log [p_\\theta(x|z)]$, and which kind of loss is commonly used in practice for the reconstruction term? MSE?\nThe real reason you use MSE and cross-entropy loss functions Modern Latent Variable Models and Variational Inference Why don’t we use MSE as a reconstruction loss for VAE ? A Hierarchical VAE is a generalization of a VAE that extends to multiple hierarchies over latent variables. A special case of HVAE is Markovian HVAE where the generative process is a Markov chain.\nVariational Diffusion Models A variational diffusion model can be regarded as a special case of a Markovian HVAE with three restrictions:\nThe latent dimension is exactly equal to the data dimension.\nThe encoder is pre-defined as a linear Gaussian.\nThe Gaussian parameters of the latent encoders vary over time in such a way that the distribution of the latent at final timestep $T$ is a standard Gaussian.\nThe variational diffusion model can be optimized by maximizing the ELBO, which can be derived as:\n$$ \\begin{align} \\log p(x) \u0026 \\geq \\mathbb{E}{q(x{1:T}|x_0)}[\\log \\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}] \\ \u0026 = \\mathbb{E}{q(x_1|x_0)}[\\log p\\theta(x_0|x_1)] - \\mathbb{E}{q(x{T-1}|x_0)}[D_{KL}(q(x_T|x_{T-1}) || p(x_T))] - \\sum_{t=1}^{T-1}\\mathbb{E}{q(x{t-1},x_{t+1}|x_0)}[D_{KL}(q(x_t|x_{t-1}) || p_\\theta(x_t|x_{t+1}))] \\end{align} $$\nCurrently, I don’t know how to derive the third term of the above equation, i.e.\n$$ q(x_{t-1}, x_t, x_{t+1}|x_0) \\overset{\\text{?}}{=} q(x_{t-1},x_{t+1}|x_0) q(x_t|x_{t-1}) $$\nAs pointed out by Calvin Luo, optimizing the ELBO using the terms above is suboptimal; because the third term is computed as an expectation over two random variables for every timestep, and the variance of its Monte Carlo estimate could potentially be higher than that only having one variable.\nUsing Markov property and Bayes rule, we have\n$$ \\begin{align} q(x_t|x_{t-1}) \u0026= q(x_t|x_{t-1}, x_0) \\ \u0026= \\frac{q(x_{t-1}|x_t,x_0) q(x_t|x_0)}{q(x_{t-1}|x_0)} \\end{align} $$\nArmed with above equation, the ELBO can be derived as:\n$$ \\begin{align} \\log p(x) \u0026 \\geq \\mathbb{E}{q(x{1:T}|x_0)}[\\log \\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}] \\ \u0026 = \\underbrace{\\mathbb{E}{q(x_1|x_0)}[\\log p\\theta(x_0|x_1)]}{\\text{reconstruction term}} - \\underbrace{D{KL}(q(x_T|x_0) || p(x_T))}{\\text{prior matching term}} - \\sum{t=2}^{T}\\underbrace{\\mathbb{E}{q(x_t|x_0)}[D{KL}(q(x_{t-1}|x_t,x_0) || p_\\theta(x_{t-1}|x_t))]}_{\\text{denoising matching term}} \\end{align} $$\nNow, to optimize the ELBO, the main difficulty comes from the denoising matching term. However, with Gaussian assumption in the variational diffusion model, a nice property is that $q(x_{t-1}|x_t, x_0)$ has a closed form: Gaussian! Let’s explain how this nice property emerges.\nStep 1. Rewrite using Bayes rule and Markov property.\n$$ q(x_{t-1}|x_t, x_0) = \\frac{q(x_t|x_{t-1}, x_0) q(x_{t-1}|x_0)}{q(x_t|x_0)} = \\frac{q(x_t|x_{t-1}) q(x_{t-1}|x_0)}{q(x_t|x_0)} $$\nStep 2. $q(x_t|x_{t-1})$ is Gaussian.\n$$ q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t)I) $$\nStep 3. $q(x_t|x_0)$ can be recursively derived through repeated applications of the reparameterization trick. And it is also Gaussian!\n$$ q(x_t|x_0) \\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t)I) $$\nStep 4. Putting above all together, we can calculate the Gaussian form of $q(x_{t-1}|x_t, x_0)$.\n$$ \\begin{align} q(x_{t-1}|x_t, x_0) \u0026 \\propto \\mathcal{N}(x_{t-1}; \\underbrace{\\frac{\\sqrt{\\alpha_t} (1-\\bar{\\alpha}{t-1}) x_t + \\sqrt{\\bar{\\alpha}{t-1}} (1-\\alpha_t) x_0}{1-\\bar{\\alpha}t}}{\\mu_q(x_t,x_0)}, \\underbrace{\\frac{(1-\\alpha_t) (1-\\bar{\\alpha}{t-1})}{1-\\bar{\\alpha}t}I}{\\sigma_q^2(t)I}) \\ \u0026 = \\mathcal{N}(x{t-1}; \\mu_q(x_t, x_0), \\sigma_q^2(t)I) \\end{align} $$\nNow we have derived the form of $q(x_{t-1}|x_t, x_0)$. To calculate the KL Divergence between $q(x_{t-1}|x_t, x_0)$ and $q(x_{t-1}|x_t)$, we need to model $q(x_{t-1}|x_t)$ first. But how? In the variational diffusion model, $q(x_{t-1}|x_t)$ is modeled as:\na Gaussian the mean $\\mu_\\theta(x_t, t)$ is parameterized by $\\theta$ and is set to the form which is similar with $\\mu_q(x_t, x_0)$: $\\mu_\\theta(x_t, t) = \\frac{\\sqrt{\\alpha_t} (1-\\bar{\\alpha}{t-1}) x_t + \\sqrt{\\bar{\\alpha}{t-1}} (1-\\alpha_t) \\hat{x}_\\theta(x_t, t)}{1-\\bar{\\alpha}_t}$ the variance is same with $q(x_{t-1}|x_t, x_0)$, i.e. $\\sigma_q^2(t)I$ $$ q(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_q^2(t)I) $$\nAs a result, we can derive the KL Divergence as:\n$$ \\begin{align} D_{KL}(q(x_{t-1}|x_t,x_0) || p_\\theta(x_{t-1}|x_t)) \u0026= \\frac{1}{2\\sigma_q^2(t)}[||\\mu_\\theta(x_t, t) - \\mu_q(x_t, x_0)||2^2] \\ \u0026 = \\frac{1}{2\\sigma_q^2(t)} \\frac{\\bar{x}{t-1}(1-\\alpha_t)^2}{(1-\\bar{\\alpha}t)^2} [||\\hat{x}\\theta(x_t, t) - x_0||_2^2] \\end{align} $$\nIn practice, $\\hat{x}_\\theta(x_t, t)$ is parameterized by a neural network.\nIn summary, we have shown that optimizing a variational diffusion model boils down to learning a neural network to predict the ground truth data from a noised version of it.\nThree Equivalent Interpretations Awesome Materials For further resources and deeper insights, check out the following (non-exhaustive) list of blogs and papers:\nJianlin Su, Diffusion models (Chinese, 中文)\nCalvin Luo, Understanding Diffusion Models: A Unified Perspective\nYang Song, Generative Modeling by Estimating Gradients of the Data Distribution\nHugging Face, The Annotated Diffusion Model\nLilian Weng, What are Diffusion Models?\nGuidance: A Cheat Code for Diffusion Models\nScore based diffusions explained in just one paragraph\nA Path to the Variational Diffusion Loss\nReferences ",
  "wordCount" : "1334",
  "inLanguage": "en",
  "datePublished": "2023-06-01T00:00:00Z",
  "dateModified": "2023-06-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Xikun Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://xikunhuang.github.io/posts/notes-on-diffusion-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Kun",
    "logo": {
      "@type": "ImageObject",
      "url": "https://xikunhuang.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://xikunhuang.github.io/" accesskey="h" title="Kun (Alt + H)">Kun</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://xikunhuang.github.io/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://xikunhuang.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://xikunhuang.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://xikunhuang.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Notes on Diffusion Models
    </h1>
    <div class="post-meta"><span title='2023-06-01 00:00:00 +0000 UTC'>June 1, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Xikun Huang&nbsp;|&nbsp;<a href="https://github.com/adityatelange/hugo-PaperMod/tree/exampleSite/content/posts/Notes%20on%20Diffusion%20Models.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#notations" aria-label="Notations">Notations</a></li>
                <li>
                    <a href="#understanding-diffusion-models-from-the-perspective-of-vae" aria-label="Understanding Diffusion Models from the Perspective of VAE">Understanding Diffusion Models from the Perspective of VAE</a><ul>
                        
                <li>
                    <a href="#evidence-lower-bound" aria-label="Evidence Lower Bound">Evidence Lower Bound</a></li>
                <li>
                    <a href="#variational-autoencoders" aria-label="Variational Autoencoders">Variational Autoencoders</a></li>
                <li>
                    <a href="#variational-diffusion-models" aria-label="Variational Diffusion Models">Variational Diffusion Models</a><ul>
                        
                <li>
                    <a href="#three-equivalent-interpretations" aria-label="Three Equivalent Interpretations">Three Equivalent Interpretations</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#awesome-materials" aria-label="Awesome Materials">Awesome Materials</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="notations">Notations<a hidden class="anchor" aria-hidden="true" href="#notations">#</a></h2>
<p>$p(z|x)$: true posterior.</p>
<p>$q_\phi(z|x)$: approximate posterior with parameters $\phi$.</p>
<p>$x_0$: true data.</p>
<p>$x_t, t \in [1,T]$: latent variables.</p>
<p>$q(x_{t}|x_{t-1})$: forward process.</p>
<p>$p(x_t|x_{t+1})$: reverse process.</p>
<h2 id="understanding-diffusion-models-from-the-perspective-of-vae">Understanding Diffusion Models from the Perspective of VAE<a hidden class="anchor" aria-hidden="true" href="#understanding-diffusion-models-from-the-perspective-of-vae">#</a></h2>
<p>This section briefly summarizes @Luo2022 with my understanding, particularly focusing on the math logic of behind the diffusion models. The original article by Calvin Luo is highly recommended! It helps a lot!</p>
<h3 id="evidence-lower-bound">Evidence Lower Bound<a hidden class="anchor" aria-hidden="true" href="#evidence-lower-bound">#</a></h3>
<p>What are likelihood-based generative models? What are autoregressive generative models? The transformer family belongs to autoregressive models.</p>
<p>For likelihood-based generative models, we want to model the true data distribution by maximizing the likelihood $p(x)$ of all observed data $x$. We can think of the data we observed as represented or generated by an associated unseen latent variable, which can be denoted by random variable $z$. There are two direct but difficult ways to compute the likelihood $p(x)$:</p>
<ul>
<li>
<p>$p(x) = \int p(x,z)dz$: intractable for complex models since it involves integrating all latent variables.</p>
</li>
<li>
<p>$p(x) = \frac{p(x,z)}{p(z|x)}$: difficult since it involves having access to a ground truth $p(z|x)$.</p>
</li>
</ul>
<blockquote>
<p>Here I have a question: How do you know the true joint distribution $p(x,z)$?</p>
</blockquote>
<p>However, we can derive a term called the <strong>E</strong>vidence <strong>L</strong>ower <strong>Bo</strong>und (ELBO) using the above two equations. So what is the ELBO? As its name suggests, it is a lower bound of the evidence. But what is the evidence? The evidence is the log likelihood of the observed data. The relationship between the evidence and the ELBO can be mathematically written as:</p>
<p>$$
\begin{align}
\log p(x) &amp;= \mathbb{E}<em>{q</em>\phi(z|x)}[\log \frac{p(x,z)}{q_\phi(z|x)}] + D_{KL}(q_\phi(z|x)||p(z|x)) \
&amp; \geq \underbrace{\mathbb{E}<em>{q</em>\phi(z|x)}[\log \frac{p(x,z)}{q_\phi(z|x)}]}_\text{ELBO}
\end{align}
$$</p>
<p>Why the ELBO is an objective we would like to maximize?</p>
<ol>
<li>
<p>The ELBO is indeed a lower bound of the evidence.</p>
</li>
<li>
<p>Any maximization of the ELBO term with respect $\phi$ necessarily invokes an equal minimization of the $D_{KL}(q_\phi(z|x)||p(z|x))$. Because the sum of the ELBO and the DL term is a constant with respect to $\phi$ (the evidence $\log p(x)$ does not depend on $\phi$).</p>
</li>
<li>
<p>As we will see below, the ELBO can be further dissect into several interpretable components which can be computed analytically or approximated using existing estimate methods.</p>
</li>
</ol>
<h3 id="variational-autoencoders">Variational Autoencoders<a hidden class="anchor" aria-hidden="true" href="#variational-autoencoders">#</a></h3>
<p>In the vanilla VAE, we directly maximize the ELBO. The ELBO can be further divided into two parts:</p>
<p>$$
\begin{equation}
\mathbb{E}<em>{q</em>\phi(z|x)}[\log \frac{p(x,z)}{q_\phi(z|x)}] = \mathbb{E}<em>{q</em>\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z))
\end{equation}
$$</p>
<p>Here $q_\phi(z|x)$ can be regarded as the encoder with parameters $\phi$ and $p_\theta(x|z)$ can be regarded as the decoder with parameters $\theta$. So there are two kinds of parameters to be optimized:</p>
<ul>
<li>Encoder parameters: $\phi$</li>
<li>Decoder parameters: $\theta$</li>
</ul>
<p>The meaning of the name of VAE is:</p>
<ul>
<li>Variational: optimize for the best $q_\phi(z|x)$ among a family of potential posterior distributions parameterized by $\phi$.</li>
<li>AutoEncoder: reminiscent of a traditional autoencoder model.</li>
</ul>
<p>How dose VAE optimize the ELBO jointly with parameters $\phi$ and $\theta$? To my understanding, VAE uses three techniques to do this:</p>
<ol>
<li>
<p>Gaussian assumption.</p>
</li>
<li>
<p>Monte Carlo estimate.</p>
</li>
<li>
<p>Reparameterization trick.</p>
</li>
</ol>
<p>The encoder $q_\phi(z|x)$ and the prior $p(z)$ are commonly chosen to be Gaussian, which makes the KL divergence term of the ELBO having closed-form expression, i.e.</p>
<p>$$
\begin{align}
q_\phi(z|x) &amp;= \mathcal{N}(z; \mu_\phi(x), \sigma_\phi^2(x)I) \
p(z) &amp;= \mathcal{N}(z; 0, I) \
D_{KL}(q_\phi(z|x) || p(z)) &amp; \text{ can be computed analytically}.
\end{align}
$$</p>
<p>The first term of the ELBO can be approximated using a Monte Carlo estimate, i.e.</p>
<p>$$
\mathbb{E}<em>{q</em>\phi(z|x)}[\log p_\theta(x|z)] \approx \sum_{l=1}^{L}\log p_{\theta}(x|z^{(l)})
$$</p>
<p>where ${z^{(l)}}<em>{l=1}^{L}$ are sampled from $q</em>\phi(z|x)$ for every observation $x$ in the dataset, and $L$ is often set to be $1$ in practice.</p>
<p>The reparameterization trick writers a random variable as a deterministic function of a noise variable, and this allows for the optimization of the non-stochatic terms through gradient descent, i.e.</p>
<p>$$
z = \bm{\mu}<em>{\bm{\phi}}(x) + \bm{\sigma}</em>{\bm{\phi}}(x)\odot\bm{\epsilon} \quad \text{with } \bm{\epsilon} \sim \mathcal{N}(\bm{\epsilon};\bm{0}, \textbf{I})
$$</p>
<p>The objective function of VAE is ELBO which contains the reconstruction term $\log [p_\theta(x|z)]$, and which kind of loss is commonly used in practice for the reconstruction term? MSE?</p>
<ul>
<li><a href="https://www.expunctis.com/2019/01/27/Loss-functions.html">The real reason you use MSE and cross-entropy loss functions</a></li>
<li><a href="https://www.youtube.com/watch?v=7Pcvdo4EJeo">Modern Latent Variable Models and Variational Inference</a></li>
<li><a href="https://github.com/pytorch/examples/issues/399">Why don&rsquo;t we use MSE as a reconstruction loss for VAE ?</a></li>
</ul>
<p>A Hierarchical VAE is a generalization of a VAE that extends to multiple hierarchies over latent variables. A special case of HVAE is Markovian HVAE where the generative process is a Markov chain.</p>
<h3 id="variational-diffusion-models">Variational Diffusion Models<a hidden class="anchor" aria-hidden="true" href="#variational-diffusion-models">#</a></h3>
<p>A variational diffusion model can be regarded as a special case of a Markovian HVAE with three restrictions:</p>
<ol>
<li>
<p>The latent dimension is exactly equal to the data dimension.</p>
</li>
<li>
<p>The encoder is pre-defined as a linear Gaussian.</p>
</li>
<li>
<p>The Gaussian parameters of the latent encoders vary over time in such a way that the distribution of the latent at final timestep $T$ is a standard Gaussian.</p>
</li>
</ol>
<p>The variational diffusion model can be optimized by maximizing the ELBO, which can be derived as:</p>
<p>$$
\begin{align}
\log p(x) &amp; \geq \mathbb{E}<em>{q(x</em>{1:T}|x_0)}[\log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}] \
&amp; = \mathbb{E}<em>{q(x_1|x_0)}[\log p</em>\theta(x_0|x_1)] - \mathbb{E}<em>{q(x</em>{T-1}|x_0)}[D_{KL}(q(x_T|x_{T-1}) || p(x_T))] - \sum_{t=1}^{T-1}\mathbb{E}<em>{q(x</em>{t-1},x_{t+1}|x_0)}[D_{KL}(q(x_t|x_{t-1}) || p_\theta(x_t|x_{t+1}))]
\end{align}
$$</p>
<blockquote>
<p>Currently, I don&rsquo;t know how to derive the third term of the above equation, i.e.</p>
</blockquote>
<p>$$
q(x_{t-1}, x_t, x_{t+1}|x_0) \overset{\text{?}}{=} q(x_{t-1},x_{t+1}|x_0) q(x_t|x_{t-1})
$$</p>
<p>As pointed out by Calvin Luo, optimizing the ELBO using the terms above is suboptimal; because the third term is computed as an expectation over two random variables for every timestep, and the variance of its Monte Carlo estimate could potentially be higher than that only having one variable.</p>
<p>Using Markov property and Bayes rule, we have</p>
<p>$$
\begin{align}
q(x_t|x_{t-1}) &amp;= q(x_t|x_{t-1}, x_0) \
&amp;= \frac{q(x_{t-1}|x_t,x_0) q(x_t|x_0)}{q(x_{t-1}|x_0)}
\end{align}
$$</p>
<p>Armed with above equation, the ELBO can be derived as:</p>
<p>$$
\begin{align}
\log p(x) &amp; \geq \mathbb{E}<em>{q(x</em>{1:T}|x_0)}[\log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}] \
&amp; = \underbrace{\mathbb{E}<em>{q(x_1|x_0)}[\log p</em>\theta(x_0|x_1)]}<em>{\text{reconstruction term}} - \underbrace{D</em>{KL}(q(x_T|x_0) || p(x_T))}<em>{\text{prior matching term}} - \sum</em>{t=2}^{T}\underbrace{\mathbb{E}<em>{q(x_t|x_0)}[D</em>{KL}(q(x_{t-1}|x_t,x_0) || p_\theta(x_{t-1}|x_t))]}_{\text{denoising matching term}}
\end{align}
$$</p>
<p>Now, to optimize the ELBO, the main difficulty comes from the denoising matching term. However, with Gaussian assumption in the variational diffusion model, a nice property is that $q(x_{t-1}|x_t, x_0)$ has a closed form: Gaussian! Let&rsquo;s explain how this nice property emerges.</p>
<p>Step 1. Rewrite using Bayes rule and Markov property.</p>
<p>$$
q(x_{t-1}|x_t, x_0) = \frac{q(x_t|x_{t-1}, x_0) q(x_{t-1}|x_0)}{q(x_t|x_0)} = \frac{q(x_t|x_{t-1}) q(x_{t-1}|x_0)}{q(x_t|x_0)}
$$</p>
<p>Step 2. $q(x_t|x_{t-1})$ is Gaussian.</p>
<p>$$
q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, (1-\alpha_t)I)
$$</p>
<p>Step 3. $q(x_t|x_0)$ can be recursively derived through repeated applications of the reparameterization trick. And it is also Gaussian!</p>
<p>$$
q(x_t|x_0) \sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)
$$</p>
<p>Step 4. Putting above all together, we can calculate the Gaussian form of $q(x_{t-1}|x_t, x_0)$.</p>
<p>$$
\begin{align}
q(x_{t-1}|x_t, x_0) &amp; \propto \mathcal{N}(x_{t-1}; \underbrace{\frac{\sqrt{\alpha_t} (1-\bar{\alpha}<em>{t-1}) x_t + \sqrt{\bar{\alpha}</em>{t-1}} (1-\alpha_t) x_0}{1-\bar{\alpha}<em>t}}</em>{\mu_q(x_t,x_0)}, \underbrace{\frac{(1-\alpha_t) (1-\bar{\alpha}<em>{t-1})}{1-\bar{\alpha}<em>t}I}</em>{\sigma_q^2(t)I}) \
&amp; = \mathcal{N}(x</em>{t-1}; \mu_q(x_t, x_0), \sigma_q^2(t)I)
\end{align}
$$</p>
<p>Now we have derived the form of $q(x_{t-1}|x_t, x_0)$. To calculate the KL Divergence between $q(x_{t-1}|x_t, x_0)$ and $q(x_{t-1}|x_t)$, we need to model $q(x_{t-1}|x_t)$ first. But how? In the variational diffusion model, $q(x_{t-1}|x_t)$ is modeled as:</p>
<ul>
<li>a Gaussian</li>
<li>the mean $\mu_\theta(x_t, t)$ is parameterized by $\theta$ and is set to the form which is similar with $\mu_q(x_t, x_0)$: $\mu_\theta(x_t, t) = \frac{\sqrt{\alpha_t} (1-\bar{\alpha}<em>{t-1}) x_t + \sqrt{\bar{\alpha}</em>{t-1}} (1-\alpha_t) \hat{x}_\theta(x_t, t)}{1-\bar{\alpha}_t}$</li>
<li>the variance is same with $q(x_{t-1}|x_t, x_0)$, i.e. $\sigma_q^2(t)I$</li>
</ul>
<p>$$
q(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_q^2(t)I)
$$</p>
<p>As a result, we can derive the KL Divergence as:</p>
<p>$$
\begin{align}
D_{KL}(q(x_{t-1}|x_t,x_0) || p_\theta(x_{t-1}|x_t)) &amp;= \frac{1}{2\sigma_q^2(t)}[||\mu_\theta(x_t, t) - \mu_q(x_t, x_0)||<em>2^2] \
&amp; = \frac{1}{2\sigma_q^2(t)} \frac{\bar{x}</em>{t-1}(1-\alpha_t)^2}{(1-\bar{\alpha}<em>t)^2} [||\hat{x}</em>\theta(x_t, t) - x_0||_2^2]
\end{align}
$$</p>
<p>In practice, $\hat{x}_\theta(x_t, t)$ is parameterized by a neural network.</p>
<p><code>In summary, we have shown that optimizing a variational diffusion model boils down to learning a neural network to predict the ground truth data from a noised version of it.</code></p>
<h4 id="three-equivalent-interpretations">Three Equivalent Interpretations<a hidden class="anchor" aria-hidden="true" href="#three-equivalent-interpretations">#</a></h4>
<h2 id="awesome-materials">Awesome Materials<a hidden class="anchor" aria-hidden="true" href="#awesome-materials">#</a></h2>
<p>For further resources and deeper insights, check out the following (non-exhaustive) list of blogs and papers:</p>
<ul>
<li>
<p>Jianlin Su, <a href="https://kexue.fm/category/Big-Data">Diffusion models</a> (Chinese, 中文)</p>
</li>
<li>
<p>Calvin Luo, <a href="https://arxiv.org/abs/2208.11970">Understanding Diffusion Models: A Unified Perspective</a></p>
</li>
<li>
<p>Yang Song, <a href="https://yang-song.net/blog/2021/score/">Generative Modeling by Estimating Gradients of the Data Distribution</a></p>
</li>
<li>
<p>Hugging Face, <a href="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion Model</a></p>
</li>
<li>
<p>Lilian Weng, <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a></p>
</li>
<li>
<p><a href="https://sander.ai/2022/05/26/guidance.html">Guidance: A Cheat Code for Diffusion Models</a></p>
</li>
<li>
<p><a href="https://kidger.site/thoughts/score-based-diffusions-explained-in-just-one-paragraph/">Score based diffusions explained in just one paragraph</a></p>
</li>
<li>
<p><a href="https://blog.alexalemi.com/diffusion.html">A Path to the Variational Diffusion Loss</a></p>
</li>
</ul>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Notes on Diffusion Models on twitter"
        href="https://twitter.com/intent/tweet/?text=Notes%20on%20Diffusion%20Models&amp;url=https%3a%2f%2fxikunhuang.github.io%2fposts%2fnotes-on-diffusion-models%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Notes on Diffusion Models on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fxikunhuang.github.io%2fposts%2fnotes-on-diffusion-models%2f&amp;title=Notes%20on%20Diffusion%20Models&amp;summary=Notes%20on%20Diffusion%20Models&amp;source=https%3a%2f%2fxikunhuang.github.io%2fposts%2fnotes-on-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Notes on Diffusion Models on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fxikunhuang.github.io%2fposts%2fnotes-on-diffusion-models%2f&title=Notes%20on%20Diffusion%20Models">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Notes on Diffusion Models on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fxikunhuang.github.io%2fposts%2fnotes-on-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Notes on Diffusion Models on whatsapp"
        href="https://api.whatsapp.com/send?text=Notes%20on%20Diffusion%20Models%20-%20https%3a%2f%2fxikunhuang.github.io%2fposts%2fnotes-on-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Notes on Diffusion Models on telegram"
        href="https://telegram.me/share/url?text=Notes%20on%20Diffusion%20Models&amp;url=https%3a%2f%2fxikunhuang.github.io%2fposts%2fnotes-on-diffusion-models%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://xikunhuang.github.io/">Kun</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
